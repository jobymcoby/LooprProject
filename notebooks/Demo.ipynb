{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PIL\n",
    "import cv2\n",
    "import scipy\n",
    "import pathlib\n",
    "import winsound\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tqdm as notebook_tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "The textile fabric database consists of 245 images of 7 different fabrics. There are 140 defect-free images, 20 for each type of fabric. With different types of defects, there are 105 images.\n",
    "\n",
    "Images have a size of 4096Ã—256 pixels. Defective images have been denominated as follows: nnnn_ddd_ff.png, where nnnn is the image number, ddd is the defect code, and ff is the fabric code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defects found: 106\n",
      "No Defects found: 141\n"
     ]
    }
   ],
   "source": [
    "dataset_folder = \"..\\\\archive\"  \n",
    "no_defect_images_folder = os.path.join(dataset_folder, \"NODefect_images\")\n",
    "defect_images_folder = os.path.join(dataset_folder, \"Defect_images\")\n",
    "mask_images_folder = os.path.join(dataset_folder, \"Mask_images\")\n",
    "\n",
    "def gather_filenames(mypath: os.path) -> list[str]:\n",
    "    filepaths = []\n",
    "    for path, _, files in os.walk(mypath):\n",
    "        for name in files:\n",
    "            filepaths.append(os.path.join(path, name))\n",
    "    return filepaths\n",
    "\n",
    "defect_file_paths = gather_filenames(defect_images_folder)\n",
    "no_defect_file_paths = gather_filenames(no_defect_images_folder)\n",
    "\n",
    "print(f'Defects found: {len(defect_file_paths)}\\nNo Defects found: {len(no_defect_file_paths)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defect codes\n",
    "DEFECT_CODES = {\n",
    "    0:  'No defect',\n",
    "    2:\t'Broken end',\n",
    "    6:\t'Broken yarn',\n",
    "    10:\t'Broken pick',\n",
    "    #16:\t'Weft curling',\n",
    "    #19:\t'Fuzzyball',\n",
    "    22: 'Cut selvage',\n",
    "    23: 'Crease',\n",
    "    25:\t'Warp ball',\n",
    "    #27: 'Knots',\n",
    "    #29:  'Contamination',\n",
    "    #30:  'Nep',\n",
    "    #36:  'Weft crack'\n",
    "}\n",
    "output_len = len(DEFECT_CODES)\n",
    "\n",
    "#encode defects to 0-12\n",
    "DEFECT_ENCODINGS = {}\n",
    "for i, key in zip(range(len(DEFECT_CODES)) , DEFECT_CODES.keys()):\n",
    "    DEFECT_ENCODINGS[key] = i\n",
    "\n",
    "def extract_labels(filepaths: list[str]) -> pd.DataFrame:\n",
    "    file_df = pd.DataFrame(filepaths, columns=['filepath'])\n",
    "\n",
    "    # x.split('\\\\') willl give the filename, then x.split('_') will give the labels\n",
    "    file_df['id'] = file_df['filepath'].map(lambda x: int(x.split('\\\\')[-1].split('_')[0]))\n",
    "    file_df['defect'] = file_df['filepath'].map(lambda x: int(x.split('\\\\')[-1].split('_')[1]))\n",
    "    file_df['fabric'] = file_df['filepath'].map(lambda x: int(x.split('\\\\')[-1].split('_')[2].split('.')[0]))\n",
    "    return file_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files names to image dataset\n",
    "defect_file_df = extract_labels(defect_file_paths)\n",
    "no_defect_file_df = extract_labels(no_defect_file_paths)\n",
    "\n",
    "# drop unwanted labels\n",
    "defect_file_df = defect_file_df.drop(defect_file_df[defect_file_df['defect'] == 16].index)\n",
    "defect_file_df = defect_file_df.drop(defect_file_df[defect_file_df['defect'] == 19].index)\n",
    "defect_file_df = defect_file_df.drop(defect_file_df[defect_file_df['defect'] == 27].index)\n",
    "defect_file_df = defect_file_df.drop(defect_file_df[defect_file_df['defect'] == 29].index)\n",
    "defect_file_df = defect_file_df.drop(defect_file_df[defect_file_df['defect'] == 30].index)\n",
    "defect_file_df = defect_file_df.drop(defect_file_df[defect_file_df['defect'] == 36].index)\n",
    "\n",
    "# Select Defects to encode\n",
    "defect_file_df['defect'] = defect_file_df['defect'].map(lambda x: DEFECT_ENCODINGS[x])\n",
    "no_defect_file_df['defect'] = no_defect_file_df['defect'].map(lambda x: DEFECT_ENCODINGS[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = defect_file_df['defect'].value_counts().sort_index().plot(kind = 'bar', title='Defect Frequency')\n",
    "plt.xticks(range(0,len(DEFECT_CODES) - 1), list(DEFECT_CODES.values())[1:])\n",
    "ax.xaxis.set_tick_params(rotation=30, labelsize=8)\n",
    "plt.gca().spines[['top', 'right',]].set_visible(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(file_path: tf.Tensor, label: tf.Tensor) -> tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n",
    "    image = tf.io.read_file(file_path)\n",
    "    image = tf.io.decode_png(image, channels=1)\n",
    "    return image, label, file_path\n",
    "\n",
    "def shuffle_split(file_ds):\n",
    "    # shuffle\n",
    "    ds_size = len(file_ds)\n",
    "    file_ds = file_ds.shuffle(ds_size, seed=3, reshuffle_each_iteration=False)\n",
    "\n",
    "    # split\n",
    "    train_split = 0.80\n",
    "    train_size = int(train_split * ds_size)\n",
    "\n",
    "    val_split = 0.10\n",
    "    val_size = int(val_split * ds_size)\n",
    "\n",
    "    test_split = 0.10\n",
    "\n",
    "    train_ds = file_ds.take(train_size)\n",
    "    val_ds = file_ds.skip(train_size).take(val_size)\n",
    "    test_ds = file_ds.skip(train_size).skip(val_size)\n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "# Files names to image dataset\n",
    "full_file_df = pd.concat([defect_file_df, no_defect_file_df])\n",
    "\n",
    "files_arr = full_file_df['filepath'].to_numpy()\n",
    "labels_arr = full_file_df['defect'].to_numpy()\n",
    "\n",
    "# Tf dataset\n",
    "file_ds = tf.data.Dataset.from_tensor_slices((files_arr, labels_arr))\n",
    "train_ds, val_ds, test_ds = shuffle_split(file_ds)\n",
    "\n",
    "# load images\n",
    "val_ds = val_ds.map(load_image)\n",
    "test_ds = test_ds.map(load_image)\n",
    "train_ds = train_ds.map(load_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate no defect from defect to augment\n",
    "no_defect_train_image_ds = train_ds.filter(lambda _, label, path: label == 0)\n",
    "defect_train_image_ds = train_ds.filter(lambda _, label, path: label > 0)\n",
    "\n",
    "rng = tf.random.Generator.from_seed(123, alg='philox')\n",
    "\n",
    "def augment(image, label, path, seed):\n",
    "    # Make new seed\n",
    "    new_seed = tf.random.split(seed, num=1)[0, :]\n",
    "    # Random brightness.\n",
    "    image = tf.image.stateless_random_brightness(\n",
    "        image, max_delta=0.5, seed=new_seed)\n",
    "    # random vertical flip\n",
    "    image = tf.image.stateless_random_flip_up_down(\n",
    "        image, seed=new_seed)\n",
    "\n",
    "    image = tf.clip_by_value(image, 0, 255)\n",
    "    return image, label, path\n",
    "\n",
    "def augment_update(x,y,z):\n",
    "    seed = rng.make_seeds(1)[:, 0]\n",
    "    x,y,z = augment(x, y, z, seed)\n",
    "    return x,y,z\n",
    "\n",
    "augments_per_image = 5\n",
    "augmented_defects_ds = (\n",
    "    defect_train_image_ds\n",
    "    .repeat(augments_per_image)\n",
    "    .map(lambda x, y, z: augment_update(x, y, z))\n",
    ")\n",
    "\n",
    "# print(f\"Number of defects before augmentation: {len(list(defect_train_image_ds.as_numpy_iterator()))}\\n\")\n",
    "defect_train_image_ds = defect_train_image_ds.concatenate(augmented_defects_ds)\n",
    "# print(f\"Number of defects after augmentation: {len(list(augmented_defects_ds.as_numpy_iterator()))}\\n\")\n",
    "# print(f\"Number of no-defects: {len(list(no_defect_train_image_ds.as_numpy_iterator()))}\\n\")\n",
    "train_ds = defect_train_image_ds.concatenate(no_defect_train_image_ds)\n",
    "# print(f\"Number of total training images: {len(list(train_ds.as_numpy_iterator()))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random crop images, we will take the highest predicted result among the batch\n",
    "# use expand dimesions to make a packet of images\n",
    "\n",
    "rng = tf.random.Generator.from_seed(123, alg='philox')\n",
    "\n",
    "def augment(image, label, path, seed):\n",
    "    # Make new seed\n",
    "    new_seed = tf.random.split(seed, num=1)[0, :]\n",
    "    # Random brightness.\n",
    "    image = tf.image.stateless_random_brightness(\n",
    "        image, max_delta=0.5, seed=new_seed)\n",
    "    # random vertical flip\n",
    "    image = tf.image.stateless_random_flip_up_down(\n",
    "        image, seed=new_seed)\n",
    "\n",
    "    image = tf.clip_by_value(image, 0, 255)\n",
    "    return image, label, path\n",
    "\n",
    "def augment_update(x,y,z):\n",
    "    seed = rng.make_seeds(1)[:, 0]\n",
    "    x,y,z = augment(x, y, z, seed)\n",
    "    return x,y,z\n",
    "\n",
    "augments_per_image = 5\n",
    "augmented_defects_ds = (\n",
    "    defect_train_image_ds\n",
    "    .repeat(augments_per_image)\n",
    "    .map(lambda x, y, z: augment_update(x, y, z))\n",
    ")\n",
    "\n",
    "# print(f\"Number of defects before augmentation: {len(list(defect_train_image_ds.as_numpy_iterator()))}\\n\")\n",
    "defect_train_image_ds = defect_train_image_ds.concatenate(augmented_defects_ds)\n",
    "# print(f\"Number of defects after augmentation: {len(list(augmented_defects_ds.as_numpy_iterator()))}\\n\")\n",
    "# print(f\"Number of no-defects: {len(list(no_defect_train_image_ds.as_numpy_iterator()))}\\n\")\n",
    "train_ds = defect_train_image_ds.concatenate(no_defect_train_image_ds)\n",
    "# print(f\"Number of total training images: {len(list(train_ds.as_numpy_iterator()))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(image, label, file_path):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "\n",
    "    image -= tf.reduce_mean(image, axis=0)\n",
    "    image = (image / 255.0)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "val_ds = val_ds.map(lambda x,y,z: normalize(x,y,z))\n",
    "test_ds = test_ds.map(lambda x,y,z: normalize(x,y,z))\n",
    "train_ds = train_ds.map(lambda x,y,z: normalize(x,y,z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder()\n",
    "enc.fit([[i] for i in list(range(len(DEFECT_CODES)))])\n",
    "\n",
    "spec = tf.TensorSpec(shape=[len(DEFECT_CODES),], dtype=tf.int64)\n",
    "@tf.py_function(Tout=spec)\n",
    "def map_defect_to_one_hot(tensor):\n",
    "    tensor = enc.transform(\n",
    "        tensor.numpy().reshape(1, 1)\n",
    "        ).toarray()[0]\n",
    "    return tensor\n",
    "\n",
    "#preprocess images\n",
    "def preprocess_img(image: tf.Tensor, label: tf.Tensor) -> tuple[tf.Tensor, tf.Tensor]:\n",
    "    image = tf.cast(image, tf.uint8)\n",
    "    image = tf.image.grayscale_to_rgb(image)\n",
    "    image = tf.image.resize(image, [224, 224])\n",
    "    # one hot encode the defects\n",
    "    label = map_defect_to_one_hot(label)\n",
    "    return image, label\n",
    "\n",
    "val_ds = val_ds.map(lambda x,y: preprocess_img(x,y))\n",
    "test_ds = test_ds.map(lambda x,y: preprocess_img(x,y))\n",
    "train_ds = train_ds.map(lambda x,y: preprocess_img(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_tensor_as_image(tensor: tf.Tensor):\n",
    "    print(tensor.shape)\n",
    "    plt.gray()\n",
    "    plt.imshow(tensor.numpy())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = val_ds.batch(4, drop_remainder=True)\n",
    "test_ds = test_ds.batch(4, drop_remainder=True)\n",
    "train_ds = train_ds.batch(4, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 224, 224, 3) (4, 7)\n",
      "82\n"
     ]
    }
   ],
   "source": [
    "for i in train_ds:\n",
    "    a, b = i[0].shape, i[1].shape \n",
    "    break\n",
    "print(a,b)\n",
    "\n",
    "def set_shapes(image, label):\n",
    "    image.set_shape(a)\n",
    "    label.set_shape(b)\n",
    "    return image, label\n",
    "\n",
    "train_ds = train_ds.map(set_shapes)\n",
    "val_ds = val_ds.map(set_shapes)\n",
    "# test_ds = test_ds.map(set_shapes)\n",
    "\n",
    "num_batches = len(list(train_ds.as_numpy_iterator()))\n",
    "# print(len(list(val_ds.as_numpy_iterator())))\n",
    "# print(len(list(test_ds.as_numpy_iterator())))\n",
    "\n",
    "train_ds = train_ds.apply(tf.data.experimental.assert_cardinality(num_batches))\n",
    "print(len(train_ds)) # NUM_BATCHES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(4, 224, 224, 3), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(4, 7), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\chris\\LooprProject\\Loopr_venv\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\chris\\LooprProject\\Loopr_venv\\lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\chris\\LooprProject\\Loopr_venv\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transfer learning on the top layers\n",
    "# Load convolutional weights that are trained on ImageNet data\n",
    "\n",
    "base_model = tf.keras.applications.resnet.ResNet152(\n",
    "    weights = 'imagenet', \n",
    "    include_top = False, \n",
    "    input_shape = (224, 224, 3)\n",
    ")\n",
    "\n",
    "# Freeze pretrained layers\n",
    "for layer in base_model.layers:\n",
    "  layer.trainable = False\n",
    "\n",
    "x = tf.keras.layers.Flatten()(base_model.output)\n",
    "x = tf.keras.layers.Dense(1000, activation='relu')(x)\n",
    "predictions = tf.keras.layers.Dense(output_len, activation = 'softmax')(x)\n",
    "\n",
    "head_model = tf.keras.Model(inputs = base_model.input, outputs = predictions)\n",
    "head_model.compile(optimizer='adam', loss= tf.keras.losses.categorical_crossentropy, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:From c:\\Users\\chris\\LooprProject\\Loopr_venv\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\chris\\LooprProject\\Loopr_venv\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "10/82 [==>...........................] - ETA: 2:08 - loss: 37.4065 - accuracy: 0.1250"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mhead_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_ds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m duration \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m  \u001b[38;5;66;03m# milliseconds\u001b[39;00m\n\u001b[0;32m      4\u001b[0m freq \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m440\u001b[39m  \u001b[38;5;66;03m# Hz\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\chris\\LooprProject\\Loopr_venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\chris\\LooprProject\\Loopr_venv\\lib\\site-packages\\keras\\src\\engine\\training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1805\u001b[0m ):\n\u001b[0;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\chris\\LooprProject\\Loopr_venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\chris\\LooprProject\\Loopr_venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\chris\\LooprProject\\Loopr_venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\chris\\LooprProject\\Loopr_venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\chris\\LooprProject\\Loopr_venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1325\u001b[0m     args,\n\u001b[0;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1327\u001b[0m     executing_eagerly)\n\u001b[0;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\chris\\LooprProject\\Loopr_venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\chris\\LooprProject\\Loopr_venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\chris\\LooprProject\\Loopr_venv\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1501\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\chris\\LooprProject\\Loopr_venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = head_model.fit(train_ds, epochs= 5, validation_data= val_ds)\n",
    "\n",
    "duration = 1000  # milliseconds\n",
    "freq = 440  # Hz\n",
    "winsound.Beep(freq, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(15,15))\n",
    "axs[0].plot(history.history['loss'])\n",
    "axs[0].plot(history.history['val_loss'])\n",
    "axs[0].title.set_text('Training Loss vs Validation Loss')\n",
    "axs[0].set_xlabel('Epochs')\n",
    "axs[0].set_ylabel('Loss')\n",
    "axs[0].legend(['Train','Val'])\n",
    "axs[1].plot(history.history['accuracy'])\n",
    "axs[1].plot(history.history['val_accuracy'])\n",
    "axs[1].title.set_text('Training Accuracy vs Validation Accuracy')\n",
    "axs[1].set_xlabel('Epochs')\n",
    "axs[1].set_ylabel('Accuracy')\n",
    "axs[1].legend(['Train', 'Val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_ds = test_ds.concatenate(val_ds)\n",
    "\n",
    "head_model.evaluate(test_ds, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = tf.keras.metrics.F1Score()\n",
    "\n",
    "test_y_true_ds = [label.numpy() for _, label in test_ds.unbatch()]\n",
    "test_y_true_ds = np.array(test_y_true_ds)\n",
    "test_y_pred_ds = head_model.predict(test_ds, verbose=2)\n",
    "\n",
    "metric.update_state(test_y_true_ds, test_y_pred_ds)\n",
    "result = metric.result()\n",
    "result.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = np.array([np.argmax(label) for label in test_y_true_ds])\n",
    "pred_labels = np.array([np.argmax(label) for label in test_y_pred_ds])\n",
    "tf.math.confusion_matrix(true_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "train_images_arr = np.array(list(train_ds.map(lambda x,y: x).as_numpy_iterator()))\n",
    "# flatten images\n",
    "train_images_arr = train_images_arr.reshape(\n",
    "    (\n",
    "        train_images_arr.shape[0], \n",
    "        train_images_arr.shape[1] * train_images_arr.shape[2]\n",
    "    )\n",
    ")\n",
    "\n",
    "pca = PCA(0.99)\n",
    "pca.fit(train_images_arr)\n",
    "PCA(copy = True, iterated_power = 'auto', n_components = 0.99, random_state = None, svd_solver = 'auto', tol = 0.0, whiten = True)\n",
    "n_components = pca.n_components_\n",
    "print(n_components)\n",
    "\n",
    "@tf.py_function(Tout=(tf.float32, tf.int64))\n",
    "def apply_pca(x,y):\n",
    "    x = x.numpy()\n",
    "    x = x.reshape([x.shape[0]*x.shape[1]])\n",
    "    x = x.reshape(1, -1)\n",
    "    return (pca.inverse_transform(pca.transform(x)), y)\n",
    "\n",
    "a = train_ds.map(apply_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Deployment/Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised or Semisupervised Learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myproject",
   "language": "python",
   "name": "myproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
