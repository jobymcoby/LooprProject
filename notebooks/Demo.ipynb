{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\chris\\LooprProject\\Loopr_venv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import PIL\n",
    "import cv2\n",
    "import scipy\n",
    "import pathlib\n",
    "import winsound\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tqdm as notebook_tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "The textile fabric database consists of 245 images of 7 different fabrics. There are 140 defect-free images, 20 for each type of fabric. With different types of defects, there are 105 images.\n",
    "\n",
    "Images have a size of 4096Ã—256 pixels. Defective images have been denominated as follows: nnnn_ddd_ff.png, where nnnn is the image number, ddd is the defect code, and ff is the fabric code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defects found: 106\n",
      "No Defects found: 141\n"
     ]
    }
   ],
   "source": [
    "dataset_folder = \"..\\\\archive\"  \n",
    "no_defect_images_folder = os.path.join(dataset_folder, \"NODefect_images\")\n",
    "defect_images_folder = os.path.join(dataset_folder, \"Defect_images\")\n",
    "mask_images_folder = os.path.join(dataset_folder, \"Mask_images\")\n",
    "\n",
    "def gather_filenames(mypath: os.path) -> list[str]:\n",
    "    filepaths = []\n",
    "    for path, _, files in os.walk(mypath):\n",
    "        for name in files:\n",
    "            filepaths.append(os.path.join(path, name))\n",
    "    return filepaths\n",
    "\n",
    "defect_file_paths = gather_filenames(defect_images_folder)\n",
    "no_defect_file_paths = gather_filenames(no_defect_images_folder)\n",
    "\n",
    "print(f'Defects found: {len(defect_file_paths)}\\nNo Defects found: {len(no_defect_file_paths)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defect codes\n",
    "DEFECT_CODES = {\n",
    "    0:  'No defect',\n",
    "    2:\t'Broken end',\n",
    "    6:\t'Broken yarn',\n",
    "    10:\t'Broken pick',\n",
    "    #16:\t'Weft curling',\n",
    "    #19:\t'Fuzzyball',\n",
    "    22: 'Cut selvage',\n",
    "    23: 'Crease',\n",
    "    25:\t'Warp ball',\n",
    "    #27: 'Knots',\n",
    "    #29:  'Contamination',\n",
    "    #30:  'Nep',\n",
    "    #36:  'Weft crack'\n",
    "}\n",
    "output_len = len(DEFECT_CODES)\n",
    "\n",
    "#encode defects to 0-12\n",
    "DEFECT_ENCODINGS = {}\n",
    "for i, key in zip(range(len(DEFECT_CODES)) , DEFECT_CODES.keys()):\n",
    "    DEFECT_ENCODINGS[key] = i\n",
    "\n",
    "def extract_labels(filepaths: list[str]) -> pd.DataFrame:\n",
    "    file_df = pd.DataFrame(filepaths, columns=['filepath'])\n",
    "\n",
    "    # x.split('\\\\') willl give the filename, then x.split('_') will give the labels\n",
    "    file_df['id'] = file_df['filepath'].map(lambda x: int(x.split('\\\\')[-1].split('_')[0]))\n",
    "    file_df['defect'] = file_df['filepath'].map(lambda x: int(x.split('\\\\')[-1].split('_')[1]))\n",
    "    file_df['fabric'] = file_df['filepath'].map(lambda x: int(x.split('\\\\')[-1].split('_')[2].split('.')[0]))\n",
    "    return file_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files names to image dataset\n",
    "defect_file_df = extract_labels(defect_file_paths)\n",
    "no_defect_file_df = extract_labels(no_defect_file_paths)\n",
    "\n",
    "# drop unwanted labels\n",
    "defect_file_df = defect_file_df.drop(defect_file_df[defect_file_df['defect'] == 16].index)\n",
    "defect_file_df = defect_file_df.drop(defect_file_df[defect_file_df['defect'] == 19].index)\n",
    "defect_file_df = defect_file_df.drop(defect_file_df[defect_file_df['defect'] == 27].index)\n",
    "defect_file_df = defect_file_df.drop(defect_file_df[defect_file_df['defect'] == 29].index)\n",
    "defect_file_df = defect_file_df.drop(defect_file_df[defect_file_df['defect'] == 30].index)\n",
    "defect_file_df = defect_file_df.drop(defect_file_df[defect_file_df['defect'] == 36].index)\n",
    "\n",
    "# Select Defects to encode\n",
    "defect_file_df['defect'] = defect_file_df['defect'].map(lambda x: DEFECT_ENCODINGS[x])\n",
    "no_defect_file_df['defect'] = no_defect_file_df['defect'].map(lambda x: DEFECT_ENCODINGS[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = defect_file_df['defect'].value_counts().sort_index().plot(kind = 'bar', title='Defect Frequency')\n",
    "plt.xticks(range(0,len(DEFECT_CODES) - 1), list(DEFECT_CODES.values())[1:])\n",
    "ax.xaxis.set_tick_params(rotation=30, labelsize=8)\n",
    "plt.gca().spines[['top', 'right',]].set_visible(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(file_path: tf.Tensor, label: tf.Tensor) -> tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n",
    "    image = tf.io.read_file(file_path)\n",
    "    image = tf.io.decode_png(image, channels=1)\n",
    "    return image, label, file_path\n",
    "\n",
    "def shuffle_split(file_ds):\n",
    "    # shuffle\n",
    "    ds_size = len(file_ds)\n",
    "    file_ds = file_ds.shuffle(ds_size, seed=3, reshuffle_each_iteration=False)\n",
    "\n",
    "    # split\n",
    "    train_split = 0.80\n",
    "    train_size = int(train_split * ds_size)\n",
    "\n",
    "    val_split = 0.10\n",
    "    val_size = int(val_split * ds_size)\n",
    "\n",
    "    test_split = 0.10\n",
    "\n",
    "    train_ds = file_ds.take(train_size)\n",
    "    val_ds = file_ds.skip(train_size).take(val_size)\n",
    "    test_ds = file_ds.skip(train_size).skip(val_size)\n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "# Files names to image dataset\n",
    "full_file_df = pd.concat([defect_file_df, no_defect_file_df])\n",
    "\n",
    "files_arr = full_file_df['filepath'].to_numpy()\n",
    "labels_arr = full_file_df['defect'].to_numpy()\n",
    "\n",
    "# Tf dataset\n",
    "file_ds = tf.data.Dataset.from_tensor_slices((files_arr, labels_arr))\n",
    "train_ds, val_ds, test_ds = shuffle_split(file_ds)\n",
    "\n",
    "# load images\n",
    "val_ds = val_ds.map(load_image)\n",
    "test_ds = test_ds.map(load_image)\n",
    "train_ds = train_ds.map(load_image)\n",
    "\n",
    "# seperate no defect from defect\n",
    "no_defect_train_image_ds = train_ds.filter(lambda _, label, path: label == 0)\n",
    "defect_train_image_ds = train_ds.filter(lambda _, label, path: label > 0)\n",
    "\n",
    "rng = tf.random.Generator.from_seed(123, alg='philox')\n",
    "\n",
    "def augment(image, label, path, seed):\n",
    "    # Make new seed\n",
    "    new_seed = tf.random.split(seed, num=1)[0, :]\n",
    "    # Random brightness.\n",
    "    image = tf.image.stateless_random_brightness(\n",
    "        image, max_delta=0.5, seed=new_seed)\n",
    "    # random vertical flip\n",
    "    image = tf.image.stateless_random_flip_up_down(\n",
    "        image, seed=new_seed)\n",
    "\n",
    "    image = tf.clip_by_value(image, 0, 255)\n",
    "    return image, label, path\n",
    "\n",
    "def augment_update(x,y,z):\n",
    "    seed = rng.make_seeds(1)[:, 0]\n",
    "    x,y,z = augment(x, y, z, seed)\n",
    "    return x,y,z\n",
    "\n",
    "augments_per_image = 5\n",
    "augmented_defects_ds = (\n",
    "    defect_train_image_ds\n",
    "    .repeat(augments_per_image)\n",
    "    .map(lambda x, y, z: augment_update(x, y, z))\n",
    ")\n",
    "\n",
    "# print(f\"Number of defects before augmentation: {len(list(defect_train_image_ds.as_numpy_iterator()))}\\n\")\n",
    "defect_train_image_ds = defect_train_image_ds.concatenate(augmented_defects_ds)\n",
    "# print(f\"Number of defects after augmentation: {len(list(augmented_defects_ds.as_numpy_iterator()))}\\n\")\n",
    "# print(f\"Number of no-defects: {len(list(no_defect_train_image_ds.as_numpy_iterator()))}\\n\")\n",
    "train_ds = defect_train_image_ds.concatenate(no_defect_train_image_ds)\n",
    "# print(f\"Number of total training images: {len(list(train_ds.as_numpy_iterator()))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(image, label, file_path):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "\n",
    "    image -= tf.reduce_mean(image, axis=0)\n",
    "    image = (image / 255.0)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "val_ds = val_ds.map(lambda x,y,z: normalize(x,y,z))\n",
    "test_ds = test_ds.map(lambda x,y,z: normalize(x,y,z))\n",
    "train_ds = train_ds.map(lambda x,y,z: normalize(x,y,z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_arr = np.array(list(train_ds.map(lambda x,y: x).as_numpy_iterator()))\n",
    "# flatten images\n",
    "train_images_arr = train_images_arr.reshape(\n",
    "    (\n",
    "        train_images_arr.shape[0], \n",
    "        train_images_arr.shape[1] * train_images_arr.shape[2]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(0.99)\n",
    "pca.fit(train_images_arr)\n",
    "PCA(copy = True, iterated_power = 'auto', n_components = 0.99, random_state = None, svd_solver = 'auto', tol = 0.0, whiten = True)\n",
    "n_components = pca.n_components_\n",
    "print(n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} ValueError: Found array with dim 3. PCA expected <= 2.\nTraceback (most recent call last):\n\n  File \"c:\\Users\\chris\\LooprProject\\Loopr_venv\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 268, in __call__\n    return func(device, token, args)\n\n  File \"c:\\Users\\chris\\LooprProject\\Loopr_venv\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 146, in __call__\n    outputs = self._call(device, args)\n\n  File \"c:\\Users\\chris\\LooprProject\\Loopr_venv\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 153, in _call\n    ret = self._func(*args)\n\n  File \"c:\\Users\\chris\\LooprProject\\Loopr_venv\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n\n  File \"C:\\Users\\chris\\AppData\\Local\\Temp\\ipykernel_18376\\1702957640.py\", line 5, in apply_pca\n    return (pca.inverse_transform(pca.transform(x)), y)\n\n  File \"c:\\Users\\chris\\LooprProject\\Loopr_venv\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 273, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n\n  File \"c:\\Users\\chris\\LooprProject\\Loopr_venv\\lib\\site-packages\\sklearn\\decomposition\\_base.py\", line 145, in transform\n    X = self._validate_data(\n\n  File \"c:\\Users\\chris\\LooprProject\\Loopr_venv\\lib\\site-packages\\sklearn\\base.py\", line 633, in _validate_data\n    out = check_array(X, input_name=\"X\", **check_params)\n\n  File \"c:\\Users\\chris\\LooprProject\\Loopr_venv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 997, in check_array\n    raise ValueError(\n\nValueError: Found array with dim 3. PCA expected <= 2.\n\n\n\t [[{{node EagerPyFunc}}]] [Op:IteratorGetNext] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (pca\u001b[38;5;241m.\u001b[39minverse_transform(pca\u001b[38;5;241m.\u001b[39mtransform(x)), y)\n\u001b[0;32m      7\u001b[0m a \u001b[38;5;241m=\u001b[39m train_ds\u001b[38;5;241m.\u001b[39mmap(apply_pca)\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m a:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(a)\n",
      "File \u001b[1;32mc:\\Users\\chris\\LooprProject\\Loopr_venv\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:810\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    809\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    811\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n\u001b[0;32m    812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\chris\\LooprProject\\Loopr_venv\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:773\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    770\u001b[0m \u001b[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;66;03m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecution_mode(context\u001b[38;5;241m.\u001b[39mSYNC):\n\u001b[1;32m--> 773\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator_get_next\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    774\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    775\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    776\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    778\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_spec\u001b[38;5;241m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\chris\\LooprProject\\Loopr_venv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3029\u001b[0m, in \u001b[0;36miterator_get_next\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   3027\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   3028\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 3029\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3030\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[0;32m   3031\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\chris\\LooprProject\\Loopr_venv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:5883\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   5881\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[0;32m   5882\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m-> 5883\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} ValueError: Found array with dim 3. PCA expected <= 2.\nTraceback (most recent call last):\n\n  File \"c:\\Users\\chris\\LooprProject\\Loopr_venv\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 268, in __call__\n    return func(device, token, args)\n\n  File \"c:\\Users\\chris\\LooprProject\\Loopr_venv\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 146, in __call__\n    outputs = self._call(device, args)\n\n  File \"c:\\Users\\chris\\LooprProject\\Loopr_venv\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 153, in _call\n    ret = self._func(*args)\n\n  File \"c:\\Users\\chris\\LooprProject\\Loopr_venv\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n\n  File \"C:\\Users\\chris\\AppData\\Local\\Temp\\ipykernel_18376\\1702957640.py\", line 5, in apply_pca\n    return (pca.inverse_transform(pca.transform(x)), y)\n\n  File \"c:\\Users\\chris\\LooprProject\\Loopr_venv\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 273, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n\n  File \"c:\\Users\\chris\\LooprProject\\Loopr_venv\\lib\\site-packages\\sklearn\\decomposition\\_base.py\", line 145, in transform\n    X = self._validate_data(\n\n  File \"c:\\Users\\chris\\LooprProject\\Loopr_venv\\lib\\site-packages\\sklearn\\base.py\", line 633, in _validate_data\n    out = check_array(X, input_name=\"X\", **check_params)\n\n  File \"c:\\Users\\chris\\LooprProject\\Loopr_venv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 997, in check_array\n    raise ValueError(\n\nValueError: Found array with dim 3. PCA expected <= 2.\n\n\n\t [[{{node EagerPyFunc}}]] [Op:IteratorGetNext] name: "
     ]
    }
   ],
   "source": [
    "@tf.py_function(Tout=(tf.float32, tf.int64))\n",
    "def apply_pca(x,y):\n",
    "    x = x.numpy()\n",
    "    x.reshape([x.shape[0],x.shape[1]])\n",
    "    return (pca.inverse_transform(pca.transform(x)), y)\n",
    "\n",
    "a = train_ds.map(apply_pca)\n",
    "for i in a:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(330, 175)\n",
      "(330, 1048576)\n"
     ]
    }
   ],
   "source": [
    "converted_data = pca.transform(train_images_arr)\n",
    "print(converted_data.shape)\n",
    "converted_data = pca.inverse_transform(converted_data)\n",
    "print(converted_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = PCA(0.96)\n",
    "# pca.fit(a)\n",
    "# PCA(copy = True, iterated_power = 'auto', n_components = 0.99, random_state = None, svd_solver = 'auto', tol = 0.0, whiten = True)\n",
    "# pca.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"â–¸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"â–¾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>PCA(n_components=320)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;&nbsp;PCA<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.decomposition.PCA.html\">?<span>Documentation for PCA</span></a><span class=\"sk-estimator-doc-link \">i<span>Not fitted</span></span></label><div class=\"sk-toggleable__content \"><pre>PCA(n_components=320)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "PCA(n_components=320)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(256)\n",
    "pca.fit(a)\n",
    "PCA(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(330, 320)\n"
     ]
    }
   ],
   "source": [
    "converted_data = pca.fit_transform(a)\n",
    "print(converted_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_data = converted_data.reshape((330, 4096, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(330, 4096, 256)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converted_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder()\n",
    "enc.fit([[i] for i in list(range(len(DEFECT_CODES)))])\n",
    "\n",
    "spec = tf.TensorSpec(shape=[len(DEFECT_CODES),], dtype=tf.int64)\n",
    "@tf.py_function(Tout=spec)\n",
    "def map_defect_to_one_hot(tensor):\n",
    "    tensor = enc.transform(\n",
    "        tensor.numpy().reshape(1, 1)\n",
    "        ).toarray()[0]\n",
    "    return tensor\n",
    "\n",
    "#preprocess images\n",
    "def preprocess_img(image: tf.Tensor, label: tf.Tensor, file_path: tf.Tensor) -> tuple[tf.Tensor, tf.Tensor]:\n",
    "    image = tf.cast(image, tf.uint8)\n",
    "    image = tf.image.grayscale_to_rgb(image)\n",
    "    image = tf.image.resize(image, [224, 224])\n",
    "    # one hot encode the defects\n",
    "    label = map_defect_to_one_hot(label)\n",
    "    return image, label\n",
    "\n",
    "# augmented_defects_ds = augmented_defects_ds.map(lambda x,y,z: preprocess_img(x,y,z))\n",
    "\n",
    "val_ds = val_ds.map(lambda x,y,z: preprocess_img(x,y,z))\n",
    "test_ds = test_ds.map(lambda x,y,z: preprocess_img(x,y,z))\n",
    "train_ds = train_ds.map(lambda x,y,z: preprocess_img(x,y,z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_tensor_as_image(tensor: tf.Tensor):\n",
    "    print(tensor.shape)\n",
    "    plt.gray()\n",
    "    plt.imshow(tensor.numpy())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(train_ds) = 185\n",
    "val_ds = val_ds.batch(4, drop_remainder=True)\n",
    "test_ds = test_ds.batch(4, drop_remainder=True)\n",
    "train_ds = train_ds.batch(4, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a,b = view_ds(train_ds)\n",
    "print(a,b)\n",
    "\n",
    "def set_shapes(image, label):\n",
    "    image.set_shape(a)\n",
    "    label.set_shape(b)\n",
    "    return image, label\n",
    "\n",
    "train_ds = train_ds.map(set_shapes)\n",
    "val_ds = val_ds.map(set_shapes)\n",
    "# test_ds = test_ds.map(set_shapes)\n",
    "\n",
    "num_batches = len(list(train_ds.as_numpy_iterator()))\n",
    "# print(len(list(val_ds.as_numpy_iterator())))\n",
    "# print(len(list(test_ds.as_numpy_iterator())))\n",
    "\n",
    "train_ds = train_ds.apply(tf.data.experimental.assert_cardinality(num_batches))\n",
    "print(len(train_ds)) # NUM_BATCHES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer learning on the top layers\n",
    "# Load convolutional weights that are trained on ImageNet data\n",
    "\n",
    "base_model = tf.keras.applications.resnet.ResNet152(\n",
    "    weights = 'imagenet', \n",
    "    include_top = False, \n",
    "    input_shape = (224, 224, 3)\n",
    ")\n",
    "\n",
    "# Freeze pretrained layers\n",
    "for layer in base_model.layers:\n",
    "  layer.trainable = False\n",
    "\n",
    "x = tf.keras.layers.Flatten()(base_model.output)\n",
    "x = tf.keras.layers.Dense(1000, activation='relu')(x)\n",
    "predictions = tf.keras.layers.Dense(output_len, activation = 'softmax')(x)\n",
    "\n",
    "head_model = tf.keras.Model(inputs = base_model.input, outputs = predictions)\n",
    "head_model.compile(optimizer='adam', loss= tf.keras.losses.categorical_crossentropy, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = head_model.fit(train_ds, epochs= 5, validation_data= val_ds)\n",
    "\n",
    "duration = 1000  # milliseconds\n",
    "freq = 440  # Hz\n",
    "winsound.Beep(freq, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(15,15))\n",
    "axs[0].plot(history.history['loss'])\n",
    "axs[0].plot(history.history['val_loss'])\n",
    "axs[0].title.set_text('Training Loss vs Validation Loss')\n",
    "axs[0].set_xlabel('Epochs')\n",
    "axs[0].set_ylabel('Loss')\n",
    "axs[0].legend(['Train','Val'])\n",
    "axs[1].plot(history.history['accuracy'])\n",
    "axs[1].plot(history.history['val_accuracy'])\n",
    "axs[1].title.set_text('Training Accuracy vs Validation Accuracy')\n",
    "axs[1].set_xlabel('Epochs')\n",
    "axs[1].set_ylabel('Accuracy')\n",
    "axs[1].legend(['Train', 'Val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_ds = test_ds.concatenate(val_ds)\n",
    "\n",
    "head_model.evaluate(test_ds, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = tf.keras.metrics.F1Score()\n",
    "\n",
    "test_y_true_ds = [label.numpy() for _, label in test_ds.unbatch()]\n",
    "test_y_true_ds = np.array(test_y_true_ds)\n",
    "test_y_pred_ds = head_model.predict(test_ds, verbose=2)\n",
    "\n",
    "metric.update_state(test_y_true_ds, test_y_pred_ds)\n",
    "result = metric.result()\n",
    "result.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = np.array([np.argmax(label) for label in test_y_true_ds])\n",
    "pred_labels = np.array([np.argmax(label) for label in test_y_pred_ds])\n",
    "tf.math.confusion_matrix(true_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Deployment/Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised or Semisupervised Learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myproject",
   "language": "python",
   "name": "myproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
